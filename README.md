# ğŸ¤Ÿ ASL Translator v2.0 - Optimized Edition

<div align="center">

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![MediaPipe](https://img.shields.io/badge/MediaPipe-0.10.8-green.svg)](https://mediapipe.dev/)
[![OpenCV](https://img.shields.io/badge/OpenCV-4.8+-red.svg)](https://opencv.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

*Real-time American Sign Language translator with ML training, photo capture, and enhanced visual feedback*

[Features](#-features) â€¢ [Installation](#-installation) â€¢ [Quick Start](#-quick-start) â€¢ [Training](#-training-guide) â€¢ [Documentation](#-documentation)

</div>

---

## ğŸš€ What's New (October 2024)

### âœ¨ Latest Features

- ğŸ“¸ **Training Photo Capture** - Automatically saves photos (with/without dots) during training
- ğŸ¨ **Enhanced Visuals** - 121 colorful tracking dots, finger labels, and hand highlighting
- ğŸ“Š **Finger Status Panel** - Real-time finger detection (UP/DOWN) during training
- âš¡ **Performance Optimized** - 2x faster FPS (40-50 FPS), improved thumb detection
- ğŸ—‚ï¸ **Better Organization** - Structured file layout with docs/ and scripts/ folders

---

## âš ï¸ MODEL TRAINING REQUIRED

> **Note**: This repository does not include pre-trained models. Train with your own gestures for personalized accuracy.

**Status**: Ready for training  
**Training Data**: Empty - Start fresh!  
**Photos**: Automatically saved to `training_photos/` during training

ğŸ“š **See [docs/MODEL_STATUS.md](docs/MODEL_STATUS.md) for detailed instructions**

---

## ğŸ“‹ Overview

ASL Translator is a real-time hand gesture recognition system with advanced visual feedback, ML training capabilities, and automatic photo documentation of training sessions.

### âœ¨ Core Features

#### Training & ML
- ğŸ¤– **Easy Training Mode** - Press letter keys + ENTER to capture samples
- ğŸ“¸ **Auto Photo Capture** - Saves training photos with dots AND clean versions
- ğŸ¯ **Bulk Training** - Automatic outlier removal (z-score method)
- ğŸ§  **ML Classifier** - Neural network (128â†’64â†’32 neurons)
- ğŸ“Š **Prediction Debug** - See top 3 predictions with confidence percentages

#### Visual Enhancements
- ğŸ¨ **121 Tracking Dots** - 21 green landmarks + 100 yellow intermediate dots
- ğŸ·ï¸ **Finger Labels** - Each fingertip labeled (Thumb, Index, Middle, Ring, Pinky)
- ğŸ“Š **Finger Status Panel** - Real-time UP/DOWN detection for each finger
- ğŸŸ¢ **Hand Highlighting** - Green bounding box around detected hand
- ğŸ’¡ **Training Hints** - Expected finger counts for A, V, W

#### Performance
- âš¡ **Optimized FPS** - 40-50 FPS (2x improvement)
- ğŸ‘ **Improved Thumb Detection** - Angle-based algorithm (90% accuracy)
- ğŸš€ **Lite MediaPipe Model** - Faster processing without accuracy loss
- ğŸ“¹ **Optimized Resolution** - 960x540 for smooth performance

#### Additional Features
- ğŸ“ **Practice Mode** - Interactive learning
- ğŸ“¹ **Video Recording** - Save signing sessions
- ğŸ”Š **Voice Output** - Text-to-speech
- ğŸ“ **Custom Dictionary** - Save frequent words
- ğŸ“Š **Performance Analytics** - Track accuracy

---

## ğŸ“ Project Structure

```
asl-translator/
â”œâ”€â”€ main.py                 # Main entry point
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ training_data.json      # Training samples (JSON)
â”œâ”€â”€ asl_model.pkl          # Trained model (generated)
â”œâ”€â”€ asl_model_scaler.pkl   # ML scaler (generated)
â”‚
â”œâ”€â”€ src/                   # Source code
â”‚   â”œâ”€â”€ asl_translator.py # Main application
â”‚   â”œâ”€â”€ hand_detector.py  # Hand tracking & visualization
â”‚   â””â”€â”€ ml_trainer.py     # ML training & prediction
â”‚
â”œâ”€â”€ docs/                  # Documentation
â”‚   â”œâ”€â”€ MODEL_STATUS.md   # Training status & guide
â”‚   â”œâ”€â”€ EASY_TRAINING.md  # Quick training guide
â”‚   â”œâ”€â”€ TRAINING_GUIDE.md # Comprehensive training
â”‚   â”œâ”€â”€ ENHANCED_VISUALS.md # Visual features
â”‚   â”œâ”€â”€ PERFORMANCE_OPTIMIZATIONS.md # Performance details
â”‚   â””â”€â”€ ... (more guides)
â”‚
â”œâ”€â”€ scripts/               # Utility scripts
â”‚   â”œâ”€â”€ diagnose_training.py # Analyze training data
â”‚   â”œâ”€â”€ QUICK_START_TRAINING.py # Training walkthrough
â”‚   â””â”€â”€ TEST_OPTIMIZATIONS.py # Test improvements
â”‚
â”œâ”€â”€ training_photos/       # Training photos (auto-generated)
â”‚   â”œâ”€â”€ A/                # Letter A photos
â”‚   â”œâ”€â”€ V/                # Letter V photos
â”‚   â””â”€â”€ W/                # Letter W photos
â”‚
â”œâ”€â”€ recordings/            # Video recordings (optional)
â””â”€â”€ documentation/         # Additional docs
```

---

## ğŸ”§ Installation

### Prerequisites

- Python 3.9 or higher
- Webcam
- macOS, Linux, or Windows

### Setup Steps

1. **Clone the repository**
```bash
git clone https://github.com/Hamdan772/asl-translator.git
cd asl-translator
```

2. **Create virtual environment** (recommended)
```bash
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Run the application**
```bash
python3 main.py
```

---

## ğŸš€ Quick Start

### First Time Setup

1. **Start the application**
```bash
python3 main.py
```

2. **Enter Training Mode**
   - Press `T` to activate training mode
   - You'll see the FINGER STATUS panel on the left

3. **Train Letter A** (closed fist)
   - Press `A`
   - Make a tight fist, thumb to the side
   - Verify FINGER STATUS shows: **Count: 0** (all fingers DOWN)
   - Press `ENTER` 20 times (slight variations each time)
   - Photos automatically saved to `training_photos/A/`

4. **Train Letter V** (peace sign âœŒï¸)
   - Press `V`
   - Spread index + middle fingers WIDE
   - Verify FINGER STATUS shows: **Count: 2** (index + middle UP)
   - Press `ENTER` 20 times
   - Photos automatically saved to `training_photos/V/`

5. **Train Letter W** (three fingers)
   - Press `W`
   - Spread index + middle + ring fingers WIDE
   - Verify FINGER STATUS shows: **Count: 3**
   - Press `ENTER` 20 times
   - Photos automatically saved to `training_photos/W/`

6. **Bulk Train the Model**
   - Press `B` to train with outlier removal
   - Wait for training to complete (~10 seconds)
   - Check terminal for accuracy percentage

7. **Test Your Model**
   - Press `ESC` to exit training mode
   - Make A, V, or W gestures
   - Watch terminal for predictions: `ğŸ” Predictions: âœ…A:89% V:8% W:3%`

---

## ğŸ“¸ Training Photo Capture

### Automatic Photo Saving

When you press `ENTER` during training, **TWO photos** are automatically saved:

1. **With Dots** (`A_001_20241024_123456_dots.jpg`)
   - Shows all 121 tracking dots
   - Green landmarks + yellow intermediate dots
   - Finger labels visible
   - Useful for debugging hand tracking

2. **Clean Version** (`A_001_20241024_123456_clean.jpg`)
   - Original camera frame without any overlays
   - Pure hand gesture
   - Useful for documentation or analysis

### Photo Organization

```
training_photos/
â”œâ”€â”€ A/
â”‚   â”œâ”€â”€ A_001_20241024_123456_dots.jpg
â”‚   â”œâ”€â”€ A_001_20241024_123456_clean.jpg
â”‚   â”œâ”€â”€ A_002_20241024_123501_dots.jpg
â”‚   â””â”€â”€ A_002_20241024_123501_clean.jpg
â”œâ”€â”€ V/
â”‚   â”œâ”€â”€ V_001_20241024_123530_dots.jpg
â”‚   â””â”€â”€ V_001_20241024_123530_clean.jpg
â””â”€â”€ W/
    â”œâ”€â”€ W_001_20241024_123600_dots.jpg
    â””â”€â”€ W_001_20241024_123600_clean.jpg
```

---

## ğŸ® Controls

### Main Controls
- `T` - Enter/exit training mode
- `M` - Train ML model (basic)
- `B` - Bulk train with outlier removal (recommended)
- `N` - Show ML statistics
- `SPACE` - Add space to text
- `BACKSPACE` - Delete last character
- `C` - Clear all text
- `ESC` - Save and quit
- `Q` - Quit without saving

### Training Mode
- `A-Z` - Select letter to train
- `ENTER` - Capture current gesture (saves photos automatically)
- `ESC` - Exit training mode

### Advanced Features
- `H` - Show/hide help guide
- `S` - Show/hide statistics
- `4` - Toggle practice mode
- `5` - Start/stop recording
- `6` - Toggle voice output
- `7` - Add word to dictionary
- `1-3` - Accept word suggestions

---

## ğŸ“Š Visual Features Explained

### FINGER STATUS Panel

During training mode, you'll see this panel on the left:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FINGER STATUS         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Thumb:  DOWN  ğŸ‘‡        â”‚  â† Gray = curled
â”‚ Index:  UP    ğŸ‘†        â”‚  â† Green = extended
â”‚ Middle: UP    ğŸ‘†        â”‚
â”‚ Ring:   DOWN  ğŸ‘‡        â”‚
â”‚ Pinky:  DOWN  ğŸ‘‡        â”‚
â”‚                         â”‚
â”‚ Count: 2                â”‚  â† Total fingers UP
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Use this to verify hand positions BEFORE capturing!**

### 121 Tracking Dots

- **21 Green Dots** (5px) - Main hand landmarks
- **100 Yellow Dots** (3px) - Intermediate tracking points
- **Cyan Lines** (3px) - Connections between landmarks
- **Purple Labels** - Finger names on fingertips

### Hand Highlighting

- **Green Box** - Auto-detected hand region
- **Semi-transparent overlay** - Better dot visibility

---

## ğŸ¯ Training Guide

### Best Practices

1. **Good Lighting** - Face a light source
2. **Clear Background** - Easier hand detection
3. **Hand Stability** - Wait for "STABLE" indicator
4. **Variety** - Slightly rotate hand between captures (Â±10Â°)
5. **Verification** - Check FINGER STATUS before pressing ENTER
6. **Quantity** - 20+ samples per letter recommended

### Common Mistakes to Avoid

âŒ **Letter A**: Fingers partially up (should be 0)  
âŒ **Letter V**: Only 1 finger or ring finger up (should be 2: index + middle)  
âŒ **Letter W**: All 4 fingers up (should be 3: index + middle + ring)  
âŒ **Capturing while "MOVING"** (wait for "STABLE")  
âŒ **Fingers close together** (SPREAD them wide for V and W)

### Using Diagnostic Tools

**Analyze your training data:**
```bash
python3 scripts/diagnose_training.py
```

This will show:
- Finger extension statistics per letter
- Problematic samples
- Recommendations for improvement

---

## ğŸ“š Documentation

### Comprehensive Guides

- **[EASY_TRAINING.md](docs/EASY_TRAINING.md)** - Quick start training guide
- **[TRAINING_GUIDE.md](docs/TRAINING_GUIDE.md)** - Detailed training instructions
- **[ENHANCED_VISUALS.md](docs/ENHANCED_VISUALS.md)** - Visual features documentation
- **[PERFORMANCE_OPTIMIZATIONS.md](docs/PERFORMANCE_OPTIMIZATIONS.md)** - Performance details
- **[TRAINING_TIPS_AVW.md](docs/TRAINING_TIPS_AVW.md)** - A/V/W specific tips
- **[VISUAL_GUIDE_AVW.md](docs/VISUAL_GUIDE_AVW.md)** - ASCII art hand positions

### Utility Scripts

- **[diagnose_training.py](scripts/diagnose_training.py)** - Analyze training data quality
- **[QUICK_START_TRAINING.py](scripts/QUICK_START_TRAINING.py)** - Interactive training guide
- **[TEST_OPTIMIZATIONS.py](scripts/TEST_OPTIMIZATIONS.py)** - Test performance improvements

---

## ğŸ”§ Technical Details

### ML Architecture

- **Model**: Multi-layer Perceptron (MLP)
- **Layers**: 128 â†’ 64 â†’ 32 neurons
- **Activation**: ReLU
- **Solver**: Adam optimizer
- **Features**: 63 (21 landmarks Ã— 3 coordinates)
- **Normalization**: StandardScaler

### Performance Metrics

| Metric | Value |
|--------|-------|
| FPS | 40-50 |
| Processing Time | ~25ms per frame |
| Thumb Accuracy | ~90% |
| Resolution | 960Ã—540 (optimized) |
| Model Complexity | Lite (MediaPipe) |

### Dependencies

- **OpenCV** 4.8+ - Video processing
- **MediaPipe** 0.10.8 - Hand tracking (21 landmarks)
- **scikit-learn** 1.3+ - ML classifier
- **NumPy** 1.26+ - Numerical operations

---

## ğŸ› Troubleshooting

### Low FPS (<30)
- Close other applications
- Improve lighting (camera works harder in dim light)
- Ensure GPU acceleration is available

### Thumb Detection Issues
- Make exaggerated thumb movements
- Ensure good lighting on hand
- Keep hand centered in green box

### Model Always Predicts Same Letter
- Check training data distribution: `python3 scripts/diagnose_training.py`
- Verify finger positions using FINGER STATUS panel
- Delete `training_data.json` and retrain carefully
- View saved photos in `training_photos/` to verify hand positions

### Photos Not Saving
- Check disk space
- Ensure write permissions in project directory
- Check terminal for error messages

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **MediaPipe** team for the hand tracking solution
- **OpenCV** community for computer vision tools
- **scikit-learn** for machine learning algorithms

---

## ğŸ“ Support

For issues, questions, or suggestions:
- ğŸ“§ Open an issue on GitHub
- ğŸ“š Check the [docs/](docs/) folder for guides
- ğŸ”§ Run diagnostic scripts in [scripts/](scripts/) folder

---

<div align="center">

**Made with â¤ï¸ for the ASL community**

â­ Star this repo if you find it useful!

</div>
