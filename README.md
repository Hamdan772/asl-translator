<div align="center">

# ğŸ¤Ÿ ASL Translator

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![MediaPipe](https://img.shields.io/badge/MediaPipe-0.10.8-green.svg)](https://mediapipe.dev/)
[![OpenCV](https://img.shields.io/badge/OpenCV-4.8+-red.svg)](https://opencv.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

*Real-time American Sign Language translator with machine learning, photo capture, and multi-modal recognition*

[Features](#-features) â€¢ [Installation](#-installation) â€¢ [Quick Start](#-quick-start) â€¢ [Keybinds](#-keybinds-reference) â€¢ [Training](#-training-guide) â€¢ [Documentation](#-documentation)

</div>

---

## âš ï¸ MODEL TRAINING REQUIRED

> **Note**: This repository does not include pre-trained models. You must train your own model with your hand gestures for personalized accuracy.

**Status**: ğŸŸ¢ Ready for training  
**Training Data**: Empty - Start fresh!  
**Photos**: Automatically saved to `training_photos/` during training

ğŸ“š **See [TRAINING_GUIDE.md](TRAINING_GUIDE.md) for detailed instructions**

---

## âœ¨ Features

### ğŸ¯ Core Recognition System
- **Real-time hand tracking** with 21 landmark points (MediaPipe)
- **Multi-modal recognition** combining 4 intelligence layers:
  - ğŸ–ï¸ **Finger state detection** - Detects which fingers are UP/DOWN
  - ğŸ§  **Neural network** - 396 features (landmarks + finger states + geometry + HOG)
  - ğŸ“ **Geometric analysis** - Finger spacing, distances, and ratios
  - ğŸ“¸ **Visual features** - HOG image analysis from photos
- **Rule-based corrections** - Auto-fixes V vs W confusion
- **Confidence scoring** - Ambiguity detection with stability bonus
- **Adaptive learning** - Train custom gestures

### ğŸ“ Training System
- **Interactive training mode** - Press T to enter, select letter, capture samples
- **Photo capture** - Automatically saves cropped hand images during training
- **Finger state recording** - Captures which fingers are extended
- **Geometric features** - Records finger positions and angles
- **Bulk training** - Removes outliers for better accuracy (Press B)
- **Debug mode** - Detailed logging for troubleshooting (Press D)

### ğŸ“Š Visual Features
- **121 visual markers** - 21 green landmarks + 100 yellow connections
- **Finger status panel** - Real-time UP/DOWN indicator for each finger
- **Confidence indicators** - Shows prediction certainty
- **Hand stability detection** - Green indicator when hand is stable
- **Session statistics** - Tracks letters, accuracy, speed

---

## ğŸš€ Installation

### Prerequisites

```bash
Python 3.9 or higher
Webcam/Camera
```

### Setup

1. **Clone the repository**
```bash
git clone https://github.com/Hamdan772/asl-translator.git
cd asl-translator
```

2. **Install dependencies**
```bash
pip install opencv-python mediapipe numpy scikit-learn
```

3. **Run the application**
```bash
python3 src/asl_translator.py
```

---

## ğŸ® Keybinds Reference

### ğŸ“ Core Controls (Always Active)

| Key | Action | Description |
|-----|--------|-------------|
| **SPACE** | Add space | Inserts a space character |
| **BACKSPACE** | Delete last character | Removes the most recent character |
| **C** | Clear all text | Wipes the entire translation buffer |
| **H** | Toggle help guide | Shows/hides the help overlay |
| **S** | Toggle statistics | Shows/hides session stats |
| **Q** | Quit without saving | Exits immediately |
| **ESC** | Save and quit | Saves text to file and exits |

### ğŸ“ Training Controls

| Key | Action | Description |
|-----|--------|-------------|
| **T** | Toggle training mode | Enters/exits learning mode |
| **0-9** | Select letters to train | Number keys cycle through letter groups (see mapping below) |
| **ENTER** | Capture training sample | Saves **auto-cropped hand photo** + gesture data |
| **M** | Train ML model | Trains neural network and **automatically removes anomalous samples** |
| **B** | Bulk train (advanced) | Alternative training method with outlier removal |
| **N** | Show ML statistics | Displays training data distribution |
| **D** | Toggle debug mode | Enables/disables detailed logging |

### ğŸ”¢ Number-to-Letter Mapping

| Number | Letters | Description |
|--------|---------|-------------|
| **0** | A â†’ K â†’ U | Press 0 repeatedly to cycle through A, K, U |
| **1** | B â†’ L â†’ V | Press 1 repeatedly to cycle through B, L, V |
| **2** | C â†’ M â†’ W | Press 2 repeatedly to cycle through C, M, W |
| **3** | D â†’ N â†’ X | Press 3 repeatedly to cycle through D, N, X |
| **4** | E â†’ O â†’ Y | Press 4 repeatedly to cycle through E, O, Y |
| **5** | F â†’ P â†’ Z | Press 5 repeatedly to cycle through F, P, Z |
| **6** | G â†’ Q | Press 6 repeatedly to cycle through G, Q |
| **7** | H â†’ R | Press 7 repeatedly to cycle through H, R |
| **8** | I â†’ S | Press 8 repeatedly to cycle through I, S |
| **9** | J â†’ T | Press 9 repeatedly to cycle through J, T |

### ğŸ¯ Training Mode Workflow

```
1. Press T          â†’ Enter training mode
2. Press 1          â†’ Selects letter B (first in group 1)
3. Press 1 again    â†’ Cycles to letter L
4. Press 1 again    â†’ Cycles to letter V âœ“
5. Make V gesture   â†’ Hold hand steady
6. Press ENTER      â†’ Captures cropped hand photo + data (repeat 15-20 times)
7. Press 2          â†’ Switch to letter C (or press again for M or W)
8. Make gesture     â†’ Capture 15-20 samples with ENTER
9. Press M          â†’ Trains model with automatic outlier removal
10. Press T         â†’ Exit training mode
11. Test gestures!  â†’ Model recognizes your trained letters
```

### ğŸ’¡ Quick Tips

- **Number Keys**: Use 0-9 to select letters (press same number to cycle)
- **Auto-Cropping**: Photos are automatically cropped to hand region only
- **Debug Mode**: Press D to see detailed photo capture logs
- **Stability**: Wait for green "stable" indicator before pressing ENTER
- **Variations**: Capture samples from different angles for better accuracy
- **Quantity**: 15-20 samples per letter recommended

---

## ğŸ“š Training Guide

### Step 1: Enter Training Mode
```
Press T â†’ See "ğŸ“ LEARNING MODE ACTIVATED" with number key guide
```

### Step 2: Select a Letter Using Number Keys
```
1. Press 1 to start with letter B (key 1 = B/L/V)
2. Press 1 again to cycle to L
3. Press 1 again to cycle to V âœ“ (victory sign)
4. You'll see: "Key 1: B â†’ L â†’ V"
            "Now training: V"
```

### Step 3: Capture Training Samples
```
1. Make the V gesture with your hand
2. Hold steady until "stable" indicator appears
3. Press ENTER to capture (auto-crops hand region!)
4. Repeat 15-20 times with slight variations
5. Photos saved to training_photos/V/
```

### Step 4: Train More Letters
```
1. Press 2 to select C/M/W group
2. Press 2 twice more to get to W
3. Make W gesture
4. Press ENTER 15-20 times
5. Continue with other number keys (0-9)
```

### Step 4: Train the Model
```
Press M â†’ Automatically removes anomalous samples and trains
Watch for: "ğŸ§¹ Outliers removed: X"
Watch for: "âœ… Training accuracy: XX%"
```

### Step 5: Test Recognition
```
1. Press T to exit training mode
2. Show your trained gestures
3. Watch real-time detection!
```

### ğŸ“¸ Automatic Photo Capture (Improved!)
During training, the system automatically:
- **Detects hand bounding box** from MediaPipe landmarks
- **Crops to hand region only** (no background clutter!)
- **Adds 40px padding** around hand for better context
- **Saves cropped JPG** to `training_photos/[LETTER]/`
- **Extracts HOG features** (324 features) for visual recognition
- Records finger states (which fingers are UP/DOWN)
- Stores geometric features (spacing, angles, ratios)

---

## ğŸ§  How It Works

### Multi-Layer Intelligence System

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 1: Finger State Detection    â”‚
â”‚ â€¢ Detects which fingers are UP      â”‚
â”‚ â€¢ 4-method voting for thumb         â”‚
â”‚ â€¢ Angle + distance for others       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 2: Neural Network (396)      â”‚
â”‚ â€¢ 63 landmark coordinates           â”‚
â”‚ â€¢ 5 finger state binaries           â”‚
â”‚ â€¢ 4 geometric measurements          â”‚
â”‚ â€¢ 324 HOG image features            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 3: Rule-Based Correction     â”‚
â”‚ â€¢ Validates finger count            â”‚
â”‚ â€¢ Auto-corrects V â†” W confusion    â”‚
â”‚ â€¢ Shows correction messages         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 4: Confidence Scoring        â”‚
â”‚ â€¢ Ambiguity detection               â”‚
â”‚ â€¢ Stability bonus (+5%)             â”‚
â”‚ â€¢ Threshold: 70% minimum            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Neural Network Architecture
```
Input: 396 features
  â†“
Hidden Layer 1: 256 neurons (ReLU)
  â†“
Hidden Layer 2: 128 neurons (ReLU)
  â†“
Hidden Layer 3: 64 neurons (ReLU)
  â†“
Hidden Layer 4: 32 neurons (ReLU)
  â†“
Output: Softmax (A-Z classification)
```

---

## ğŸ“ Project Structure

```
asl-translator/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ asl_translator.py      # Main application (1466 lines)
â”‚   â”œâ”€â”€ hand_detector.py       # Hand tracking & finger detection (623 lines)
â”‚   â”œâ”€â”€ ml_trainer.py          # ML training & prediction (668 lines)
â”‚   â””â”€â”€ finger_matcher.py      # Advanced pattern matching (249 lines)
â”œâ”€â”€ training_photos/           # Auto-saved hand images (created during training)
â”œâ”€â”€ training_data.json         # Training samples with finger states
â”œâ”€â”€ TRAINING_GUIDE.md         # Detailed training instructions
â”œâ”€â”€ RELEASE_NOTES.md          # Release documentation
â””â”€â”€ README.md                 # This file
```

---

## ğŸ› Troubleshooting

### Photos Not Saving?
1. Enable debug mode (press **D**)
2. Check console for error messages
3. Verify camera permissions
4. Ensure `training_photos/` directory exists

### V and W Confused?
1. Train 20+ samples for each letter
2. Make clear finger separation
3. Check FINGER STATUS panel shows correct count
   - **V** = 2 fingers (Index + Middle)
   - **W** = 3 fingers (Index + Middle + Ring)

### Low Recognition Confidence?
1. Hold hand more stable
2. Improve lighting conditions
3. Keep hand centered in frame
4. Train more sample variations
5. Press **N** to check training statistics

### Camera Not Working?
1. Close other apps using the camera
2. Check system camera permissions
3. Restart the application
4. Try a different USB port (external webcam)

---

## ğŸ¯ Tips for Best Accuracy

1. **Lighting**: Use bright, even lighting
2. **Background**: Plain background reduces noise
3. **Distance**: Keep hand at consistent distance from camera
4. **Stability**: Hold hand still when capturing (wait for green indicator)
5. **Variation**: Train samples from different angles and positions
6. **Quantity**: 15-20 samples minimum per letter
7. **Quality**: Make clear, distinct gestures
8. **Consistency**: Use same hand and orientation

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

### Areas for Contribution
- Pre-trained model for A-Z letters
- Additional ASL signs and words
- Improved detection algorithms
- Dynamic gesture recognition (moving signs)
- Multi-hand support
- Performance optimizations
- Better UI/UX

---

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## ğŸ™ Acknowledgments

- **MediaPipe** - Google's hand tracking solution
- **OpenCV** - Computer vision library  
- **scikit-learn** - Machine learning toolkit
- ASL community for gesture references

---

## ğŸ“§ Contact

**Hamdan** - [@Hamdan772](https://github.com/Hamdan772)

**Project Link**: [https://github.com/Hamdan772/asl-translator](https://github.com/Hamdan772/asl-translator)

---

<div align="center">

### â­ Star this repo if you find it helpful!

### ğŸ› Report bugs in the [Issues](https://github.com/Hamdan772/asl-translator/issues) section

### ğŸ’¡ Suggest features via [Pull Requests](https://github.com/Hamdan772/asl-translator/pulls)

---

**Made with â¤ï¸ for the ASL community**

</div>
