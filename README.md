# ğŸ¤Ÿ ASL Translator - Advanced Real-Time Sign Language Recognition

<div align="center">

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![MediaPipe](https://img.shields.io/badge/MediaPipe-0.10.8-green.svg)](https://mediapipe.dev/)
[![OpenCV](https://img.shields.io/badge/OpenCV-4.8+-red.svg)](https://opencv.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

*Real-time American Sign Language translator with machine learning, photo capture, and multi-modal recognition*

[Features](#-features) â€¢ [Installation](#-installation) â€¢ [Quick Start](#-quick-start) â€¢ [Keybinds](#-keybinds-reference) â€¢ [Training](#-training-guide) â€¢ [Documentation](#-documentation)

</div>

---

## âš ï¸ MODEL TRAINING REQUIRED

> **Note**: This repository does not include pre-trained models. You must train your own model with your hand gestures for personalized accuracy.

**Status**: ğŸŸ¢ Ready for training  
**Training Data**: Empty - Start fresh!  
**Photos**: Automatically saved to `training_photos/` during training

ğŸ“š **See [TRAINING_GUIDE.md](TRAINING_GUIDE.md) for detailed instructions**

---

## âœ¨ Features

### ğŸ¯ Core Recognition System
- **Real-time hand tracking** with 21 landmark points (MediaPipe)
- **Multi-modal recognition** combining 4 intelligence layers:
  - ğŸ–ï¸ **Finger state detection** - Detects which fingers are UP/DOWN
  - ğŸ§  **Neural network** - 396 features (landmarks + finger states + geometry + HOG)
  - ğŸ“ **Geometric analysis** - Finger spacing, distances, and ratios
  - ğŸ“¸ **Visual features** - HOG image analysis from photos
- **Rule-based corrections** - Auto-fixes V vs W confusion
- **Confidence scoring** - Ambiguity detection with stability bonus
- **Adaptive learning** - Train custom gestures

### ğŸ“ Training System
- **Interactive training mode** - Press T to enter, select letter, capture samples
- **Photo capture** - Automatically saves cropped hand images during training
- **Finger state recording** - Captures which fingers are extended
- **Geometric features** - Records finger positions and angles
- **Bulk training** - Removes outliers for better accuracy (Press B)
- **Debug mode** - Detailed logging for troubleshooting (Press D)

### ğŸ“Š Visual Features
- **121 visual markers** - 21 green landmarks + 100 yellow connections
- **Finger status panel** - Real-time UP/DOWN indicator for each finger
- **Confidence indicators** - Shows prediction certainty
- **Hand stability detection** - Green indicator when hand is stable
- **Session statistics** - Tracks letters, accuracy, speed

---

## ğŸš€ Installation

### Prerequisites

```bash
Python 3.9 or higher
Webcam/Camera
```

### Setup

1. **Clone the repository**
```bash
git clone https://github.com/Hamdan772/asl-translator.git
cd asl-translator
```

2. **Install dependencies**
```bash
pip install opencv-python mediapipe numpy scikit-learn
```

3. **Run the application**
```bash
python3 src/asl_translator.py
```

---

## ğŸ® Keybinds Reference

### ğŸ“ Core Controls (Always Active)

| Key | Action | Description |
|-----|--------|-------------|
| **SPACE** | Add space | Inserts a space character |
| **BACKSPACE** | Delete last character | Removes the most recent character |
| **C** | Clear all text | Wipes the entire translation buffer |
| **H** | Toggle help guide | Shows/hides the help overlay |
| **S** | Toggle statistics | Shows/hides session stats |
| **Q** | Quit without saving | Exits immediately |
| **ESC** | Save and quit | Saves text to file and exits |

### ğŸ“ Training Controls

| Key | Action | Description |
|-----|--------|-------------|
| **T** | Toggle training mode | Enters/exits learning mode |
| **A-Z** | Select letter to train | Choose which letter to capture (in training mode) |
| **ENTER** | Capture training sample | Saves current hand gesture with photo |
| **M** | Train ML model (standard) | Trains neural network with all samples |
| **B** | Bulk train (advanced) | Trains model after removing outlier samples |
| **N** | Show ML statistics | Displays training data distribution |
| **D** | Toggle debug mode | Enables/disables detailed logging |

### ğŸ¯ Training Mode Workflow

```
1. Press T          â†’ Enter training mode
2. Press V          â†’ Select letter V
3. Make V gesture   â†’ Hold hand steady
4. Press ENTER      â†’ Capture sample (repeat 15-20 times)
5. Press W          â†’ Switch to letter W
6. Make W gesture   â†’ Capture 15-20 samples
7. Press M          â†’ Train the model
8. Press T          â†’ Exit training mode
9. Test gestures!   â†’ Model recognizes V and W
```

### ğŸ’¡ Quick Tips

- **Training Mode**: Stay in training mode when switching between letters (just press different letter keys)
- **Debug Mode**: Press D to see detailed photo capture logs
- **Stability**: Wait for green "stable" indicator before pressing ENTER
- **Variations**: Capture samples from different angles for better accuracy
- **Quantity**: 15-20 samples per letter recommended

---

## ğŸ“š Training Guide

### Step 1: Enter Training Mode
```
Press T â†’ See "ğŸ“ LEARNING MODE ACTIVATED"
```

### Step 2: Train Your First Letter
```
1. Press V (for victory sign)
2. Make the V gesture with your hand
3. Hold steady until "stable" indicator appears
4. Press ENTER to capture
5. Repeat 15-20 times with slight variations
```

### Step 3: Train More Letters
```
1. Press W (without exiting training mode)
2. Make the W gesture
3. Press ENTER 15-20 times
4. Continue with other letters (A, B, C, etc.)
```

### Step 4: Train the Model
```
Press M â†’ Wait for training completion
Watch for: "âœ… Training accuracy: XX%"
```

### Step 5: Test Recognition
```
1. Press T to exit training mode
2. Show your trained gestures
3. Watch real-time detection!
```

### ğŸ“¸ Photo Capture
During training, the system automatically:
- Captures cropped hand images
- Saves to `training_photos/[LETTER]/`
- Records finger states (which fingers are UP/DOWN)
- Stores geometric features (spacing, angles)

---

## ğŸ§  How It Works

### Multi-Layer Intelligence System

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 1: Finger State Detection    â”‚
â”‚ â€¢ Detects which fingers are UP      â”‚
â”‚ â€¢ 4-method voting for thumb         â”‚
â”‚ â€¢ Angle + distance for others       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 2: Neural Network (396)      â”‚
â”‚ â€¢ 63 landmark coordinates           â”‚
â”‚ â€¢ 5 finger state binaries           â”‚
â”‚ â€¢ 4 geometric measurements          â”‚
â”‚ â€¢ 324 HOG image features            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 3: Rule-Based Correction     â”‚
â”‚ â€¢ Validates finger count            â”‚
â”‚ â€¢ Auto-corrects V â†” W confusion    â”‚
â”‚ â€¢ Shows correction messages         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 4: Confidence Scoring        â”‚
â”‚ â€¢ Ambiguity detection               â”‚
â”‚ â€¢ Stability bonus (+5%)             â”‚
â”‚ â€¢ Threshold: 70% minimum            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Neural Network Architecture
```
Input: 396 features
  â†“
Hidden Layer 1: 256 neurons (ReLU)
  â†“
Hidden Layer 2: 128 neurons (ReLU)
  â†“
Hidden Layer 3: 64 neurons (ReLU)
  â†“
Hidden Layer 4: 32 neurons (ReLU)
  â†“
Output: Softmax (A-Z classification)
```

---

## ğŸ“ Project Structure

```
asl-translator/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ asl_translator.py      # Main application (1466 lines)
â”‚   â”œâ”€â”€ hand_detector.py       # Hand tracking & finger detection (623 lines)
â”‚   â”œâ”€â”€ ml_trainer.py          # ML training & prediction (668 lines)
â”‚   â””â”€â”€ finger_matcher.py      # Advanced pattern matching (249 lines)
â”œâ”€â”€ training_photos/           # Auto-saved hand images (created during training)
â”œâ”€â”€ training_data.json         # Training samples with finger states
â”œâ”€â”€ TRAINING_GUIDE.md         # Detailed training instructions
â”œâ”€â”€ RELEASE_NOTES.md          # Release documentation
â””â”€â”€ README.md                 # This file
```

---

## ğŸ› Troubleshooting

### Photos Not Saving?
1. Enable debug mode (press **D**)
2. Check console for error messages
3. Verify camera permissions
4. Ensure `training_photos/` directory exists

### V and W Confused?
1. Train 20+ samples for each letter
2. Make clear finger separation
3. Check FINGER STATUS panel shows correct count
   - **V** = 2 fingers (Index + Middle)
   - **W** = 3 fingers (Index + Middle + Ring)

### Low Recognition Confidence?
1. Hold hand more stable
2. Improve lighting conditions
3. Keep hand centered in frame
4. Train more sample variations
5. Press **N** to check training statistics

### Camera Not Working?
1. Close other apps using the camera
2. Check system camera permissions
3. Restart the application
4. Try a different USB port (external webcam)

---

## ğŸ¯ Tips for Best Accuracy

1. **Lighting**: Use bright, even lighting
2. **Background**: Plain background reduces noise
3. **Distance**: Keep hand at consistent distance from camera
4. **Stability**: Hold hand still when capturing (wait for green indicator)
5. **Variation**: Train samples from different angles and positions
6. **Quantity**: 15-20 samples minimum per letter
7. **Quality**: Make clear, distinct gestures
8. **Consistency**: Use same hand and orientation

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

### Areas for Contribution
- Pre-trained model for A-Z letters
- Additional ASL signs and words
- Improved detection algorithms
- Dynamic gesture recognition (moving signs)
- Multi-hand support
- Performance optimizations
- Better UI/UX

---

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## ğŸ™ Acknowledgments

- **MediaPipe** - Google's hand tracking solution
- **OpenCV** - Computer vision library  
- **scikit-learn** - Machine learning toolkit
- ASL community for gesture references

---

## ğŸ“§ Contact

**Hamdan** - [@Hamdan772](https://github.com/Hamdan772)

**Project Link**: [https://github.com/Hamdan772/asl-translator](https://github.com/Hamdan772/asl-translator)

---

<div align="center">

### â­ Star this repo if you find it helpful!

### ğŸ› Report bugs in the [Issues](https://github.com/Hamdan772/asl-translator/issues) section

### ğŸ’¡ Suggest features via [Pull Requests](https://github.com/Hamdan772/asl-translator/pulls)

---

**Made with â¤ï¸ for the ASL community**

</div>
